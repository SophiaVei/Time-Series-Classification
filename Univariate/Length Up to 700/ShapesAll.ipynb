{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset: ShapesAll, Dimensions: 1, Length:\t512, Train Size: 600, Test Size: 600, Classes: 60"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9946d3a303d05bc"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:19.272900900Z",
     "start_time": "2024-01-15T23:28:13.684594300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "from sklearn.metrics import precision_score, f1_score, roc_auc_score\n",
    "import time\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from collections import Counter\n",
    "from memory_profiler import memory_usage\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Deep Learning:\n",
    "from aeon.classification.deep_learning.mlp import MLPClassifier\n",
    "from aeon.classification.deep_learning.cnn import CNNClassifier\n",
    "from aeon.classification.deep_learning.fcn import FCNClassifier\n",
    "from sktime.classification.deep_learning.mcdcnn import MCDCNNClassifier\n",
    "\n",
    "# Dictionary-based:\n",
    "from aeon.classification.dictionary_based import (BOSSEnsemble, ContractableBOSS, IndividualBOSS,\n",
    "                                                  TemporalDictionaryEnsemble, IndividualTDE, WEASEL, MUSE)\n",
    "\n",
    "# Distance-based:\n",
    "from aeon.classification.distance_based import ShapeDTW, KNeighborsTimeSeriesClassifier\n",
    "\n",
    "# Feature-based:\n",
    "from aeon.classification.feature_based import Catch22Classifier, FreshPRINCEClassifier\n",
    "\n",
    "# Interval-based\n",
    "from aeon.classification.interval_based import (CanonicalIntervalForestClassifier, DrCIFClassifier,\n",
    "                                                SupervisedTimeSeriesForest, TimeSeriesForestClassifier)\n",
    "\n",
    "# Kernel-based:\n",
    "from aeon.classification.convolution_based import RocketClassifier, Arsenal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:22.312575700Z",
     "start_time": "2024-01-15T23:28:19.259379700Z"
    }
   },
   "id": "1a701669795a2ba3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each time series: 512\n",
      "Train size: 600\n",
      "Test size: 600\n",
      "Training set class distribution: Counter({'1': 10, '2': 10, '3': 10, '4': 10, '5': 10, '6': 10, '7': 10, '8': 10, '9': 10, '10': 10, '11': 10, '12': 10, '13': 10, '14': 10, '15': 10, '16': 10, '17': 10, '18': 10, '19': 10, '20': 10, '21': 10, '22': 10, '23': 10, '24': 10, '25': 10, '26': 10, '27': 10, '28': 10, '29': 10, '60': 10, '30': 10, '31': 10, '32': 10, '33': 10, '34': 10, '59': 10, '35': 10, '36': 10, '37': 10, '38': 10, '39': 10, '40': 10, '41': 10, '42': 10, '43': 10, '44': 10, '45': 10, '46': 10, '47': 10, '48': 10, '49': 10, '50': 10, '51': 10, '52': 10, '53': 10, '54': 10, '55': 10, '56': 10, '57': 10, '58': 10})\n",
      "Test set class distribution: Counter({'1': 10, '2': 10, '3': 10, '4': 10, '5': 10, '6': 10, '7': 10, '8': 10, '9': 10, '10': 10, '11': 10, '12': 10, '13': 10, '14': 10, '15': 10, '16': 10, '17': 10, '18': 10, '19': 10, '20': 10, '21': 10, '22': 10, '23': 10, '24': 10, '25': 10, '26': 10, '27': 10, '28': 10, '29': 10, '60': 10, '30': 10, '31': 10, '32': 10, '33': 10, '34': 10, '59': 10, '35': 10, '36': 10, '37': 10, '38': 10, '39': 10, '40': 10, '41': 10, '42': 10, '43': 10, '44': 10, '45': 10, '46': 10, '47': 10, '48': 10, '49': 10, '50': 10, '51': 10, '52': 10, '53': 10, '54': 10, '55': 10, '56': 10, '57': 10, '58': 10})\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ShapesAll\"  # Change this to match your dataset name\n",
    "\n",
    "# Load the dataset\n",
    "X_train_raw, y_train = load_UCR_UEA_dataset(\"ShapesAll\", split=\"train\", return_X_y=True)\n",
    "X_test_raw, y_test = load_UCR_UEA_dataset(\"ShapesAll\", split=\"test\", return_X_y=True)\n",
    "\n",
    "# Print dataset sizes and class distribution\n",
    "print(\"Length of each time series:\", X_train_raw.iloc[0, 0].size)\n",
    "print(\"Train size:\", len(y_train))\n",
    "print(\"Test size:\", len(y_test))\n",
    "print(\"Training set class distribution:\", Counter(y_train))\n",
    "print(\"Test set class distribution:\", Counter(y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:22.827661300Z",
     "start_time": "2024-01-15T23:28:22.321559700Z"
    }
   },
   "id": "a046e173fe4e5c64"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Function to convert DataFrame to 2D numpy array\n",
    "def dataframe_to_2darray(df):\n",
    "    num_samples = df.shape[0]\n",
    "    num_timesteps = len(df.iloc[0, 0])\n",
    "    array_2d = np.empty((num_samples, num_timesteps))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        array_2d[i, :] = df.iloc[i, 0]\n",
    "\n",
    "    return array_2d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:22.841661500Z",
     "start_time": "2024-01-15T23:28:22.825663100Z"
    }
   },
   "id": "8af5e6e8d710e2de"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Convert and preprocess the data\n",
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_train_processed = scaler.fit_transform(dataframe_to_2darray(X_train_raw))\n",
    "X_test_processed = scaler.transform(dataframe_to_2darray(X_test_raw))  # Use the same scaler to transform test data\n",
    "\n",
    "# Flatten each time series into a one-dimensional array for classifiers that require flat features\n",
    "X_train_flat = X_train_processed.reshape((X_train_processed.shape[0], -1))\n",
    "X_test_flat = X_test_processed.reshape((X_test_processed.shape[0], -1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:23.070564100Z",
     "start_time": "2024-01-15T23:28:22.839663300Z"
    }
   },
   "id": "bfa292d549a8d834"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Check for class imbalance\n",
    "class_distribution = Counter(y_train)\n",
    "min_class_size = min(class_distribution.values())\n",
    "max_class_size = max(class_distribution.values())\n",
    "imbalance_ratio = min_class_size / max_class_size\n",
    "imbalance_threshold = 0.5\n",
    "\n",
    "# Flag to indicate whether resampling was done\n",
    "resampling_done = False\n",
    "\n",
    "# Initialize resampled data with original data\n",
    "X_train_flat_resampled, y_train_resampled = X_train_flat, y_train\n",
    "\n",
    "# Apply oversampling if there is class imbalance\n",
    "if imbalance_ratio < imbalance_threshold:\n",
    "    print(\"Class imbalance detected. Applying RandomOverSampler...\")\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_train_flat_resampled, y_train_resampled = ros.fit_resample(X_train_flat, y_train)\n",
    "    resampling_done = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:23.086751800Z",
     "start_time": "2024-01-15T23:28:23.077462900Z"
    }
   },
   "id": "95e7ba5e91581fe2"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define a list of classifiers\n",
    "classifiers = [MLPClassifier(), CNNClassifier(), FCNClassifier(), MCDCNNClassifier(),\n",
    "               BOSSEnsemble(feature_selection='random'),ContractableBOSS(feature_selection='random'),\n",
    "               IndividualBOSS(feature_selection='random'), TemporalDictionaryEnsemble(),IndividualTDE(),\n",
    "               WEASEL(support_probabilities=True,feature_selection='random'),\n",
    "               MUSE(support_probabilities=True, feature_selection='random'),\n",
    "               ShapeDTW(), KNeighborsTimeSeriesClassifier(), Catch22Classifier(), FreshPRINCEClassifier(),\n",
    "               SupervisedTimeSeriesForest(), TimeSeriesForestClassifier(),\n",
    "               CanonicalIntervalForestClassifier(), DrCIFClassifier(), RocketClassifier(), Arsenal()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:29.858268200Z",
     "start_time": "2024-01-15T23:28:23.090753200Z"
    }
   },
   "id": "d3aa75d53b78257e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Initialize lists to store results\n",
    "results = {\"Classifier\": [], \"Execution Time\": [], \"Memory Usage\": [], \"Precision\": [], \"Accuracy\": [],\n",
    "           \"F1 Score\": [], \"ROC-AUC Score (Macro)\": [], \"ROC-AUC Score (Micro)\": [], \"Confusion Matrix\": []}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:29.908582500Z",
     "start_time": "2024-01-15T23:28:29.869149400Z"
    }
   },
   "id": "a8ef6f80d0aaaec1"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Function to evaluate classifier\n",
    "def evaluate_classifier(classifier, X_train, X_test, y_train, y_test):\n",
    "    # Inner function to include both fitting and prediction for memory profiling\n",
    "    def fit_and_predict():\n",
    "        classifier.fit(X_train, y_train)\n",
    "        return classifier.predict(X_test)\n",
    "\n",
    "    # Measure execution time and memory usage for fitting and predicting\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((fit_and_predict,), interval=0.1, include_children=True, retval=True)\n",
    "    execution_time = time.time() - start_time\n",
    "    max_mem_usage = max(mem_usage[0]) - min(mem_usage[0])  # mem_usage[0] contains the memory usage\n",
    "    predicted_labels = mem_usage[1]  # mem_usage[1] contains the return value from fit_and_predict\n",
    "\n",
    "    # Proceed with the rest of the evaluation\n",
    "    precision = precision_score(y_test, predicted_labels, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, predicted_labels)\n",
    "    f1_score_val = f1_score(y_test, predicted_labels, average='weighted')\n",
    "    confusion = confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "    # If the classifier supports probability estimates, calculate ROC AUC scores\n",
    "    roc_auc_macro = roc_auc_micro = None\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(X_test)\n",
    "        roc_auc_macro = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n",
    "        roc_auc_micro = roc_auc_score(y_test, y_prob, multi_class='ovr', average='micro')\n",
    "\n",
    "    # Return all the metrics including memory usage\n",
    "    return execution_time, max_mem_usage, precision, accuracy, f1_score_val, roc_auc_macro, roc_auc_micro, confusion"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:29.920584300Z",
     "start_time": "2024-01-15T23:28:29.902583500Z"
    }
   },
   "id": "b5304236c7ee5c6b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Preparing to plot ROC-AUC curves\n",
    "fpr_dict = {}\n",
    "tpr_dict = {}\n",
    "roc_auc_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-15T23:28:29.955586600Z",
     "start_time": "2024-01-15T23:28:29.925586700Z"
    }
   },
   "id": "5f399b5d4239d439"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 3ms/step\n",
      "27/38 [====================>.........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step\n",
      "MLPClassifier Execution Time: 1092.67s\n",
      "MLPClassifier Memory Usage: 119.63 MB\n",
      "MLPClassifier Precision: 0.42\n",
      "MLPClassifier Accuracy: 0.46\n",
      "MLPClassifier F1 Score: 0.38\n",
      "MLPClassifier ROC-AUC Score (Macro): 0.90\n",
      "MLPClassifier ROC-AUC Score (Micro): 0.91\n",
      "38/38 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step\n",
      "38/38 [==============================] - 0s 4ms/step\n",
      "CNNClassifier Execution Time: 434.09s\n",
      "CNNClassifier Memory Usage: 47.54 MB\n",
      "CNNClassifier Precision: 0.69\n",
      "CNNClassifier Accuracy: 0.68\n",
      "CNNClassifier F1 Score: 0.66\n",
      "CNNClassifier ROC-AUC Score (Macro): 0.89\n",
      "CNNClassifier ROC-AUC Score (Micro): 0.89\n",
      "38/38 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 2s 61ms/step\n",
      "38/38 [==============================] - 2s 59ms/step\n",
      "FCNClassifier Execution Time: 19986.30s\n",
      "FCNClassifier Memory Usage: 512.89 MB\n",
      "FCNClassifier Precision: 0.92\n",
      "FCNClassifier Accuracy: 0.91\n",
      "FCNClassifier F1 Score: 0.91\n",
      "FCNClassifier ROC-AUC Score (Macro): 0.99\n",
      "FCNClassifier ROC-AUC Score (Micro): 0.99\n",
      "38/38 [==============================] - 2s 60ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step\n",
      "29/38 [=====================>........] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 4ms/step\n",
      "MCDCNNClassifier Execution Time: 57.72s\n",
      "MCDCNNClassifier Memory Usage: 72.51 MB\n",
      "MCDCNNClassifier Precision: 0.45\n",
      "MCDCNNClassifier Accuracy: 0.43\n",
      "MCDCNNClassifier F1 Score: 0.42\n",
      "MCDCNNClassifier ROC-AUC Score (Macro): 0.81\n",
      "MCDCNNClassifier ROC-AUC Score (Micro): 0.82\n",
      "38/38 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOSSEnsemble Execution Time: 496.40s\n",
      "BOSSEnsemble Memory Usage: 143.21 MB\n",
      "BOSSEnsemble Precision: 0.47\n",
      "BOSSEnsemble Accuracy: 0.38\n",
      "BOSSEnsemble F1 Score: 0.38\n",
      "BOSSEnsemble ROC-AUC Score (Macro): 0.80\n",
      "BOSSEnsemble ROC-AUC Score (Micro): 0.80\n",
      "ContractableBOSS Execution Time: 149.66s\n",
      "ContractableBOSS Memory Usage: 105.89 MB\n",
      "ContractableBOSS Precision: 0.83\n",
      "ContractableBOSS Accuracy: 0.81\n",
      "ContractableBOSS F1 Score: 0.80\n",
      "ContractableBOSS ROC-AUC Score (Macro): 0.98\n",
      "ContractableBOSS ROC-AUC Score (Micro): 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sophi\\PycharmProjects\\TimeSeriesClassification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndividualBOSS Execution Time: 1.55s\n",
      "IndividualBOSS Memory Usage: 46.77 MB\n",
      "IndividualBOSS Precision: 0.09\n",
      "IndividualBOSS Accuracy: 0.10\n",
      "IndividualBOSS F1 Score: 0.09\n",
      "IndividualBOSS ROC-AUC Score (Macro): 0.54\n",
      "IndividualBOSS ROC-AUC Score (Micro): 0.54\n",
      "TemporalDictionaryEnsemble Execution Time: 3012.99s\n",
      "TemporalDictionaryEnsemble Memory Usage: 876.09 MB\n",
      "TemporalDictionaryEnsemble Precision: 0.94\n",
      "TemporalDictionaryEnsemble Accuracy: 0.93\n",
      "TemporalDictionaryEnsemble F1 Score: 0.92\n",
      "TemporalDictionaryEnsemble ROC-AUC Score (Macro): 0.99\n",
      "TemporalDictionaryEnsemble ROC-AUC Score (Micro): 0.99\n",
      "IndividualTDE Execution Time: 27.55s\n",
      "IndividualTDE Memory Usage: 51.04 MB\n",
      "IndividualTDE Precision: 0.71\n",
      "IndividualTDE Accuracy: 0.65\n",
      "IndividualTDE F1 Score: 0.63\n",
      "IndividualTDE ROC-AUC Score (Macro): 0.82\n",
      "IndividualTDE ROC-AUC Score (Micro): 0.82\n",
      "WEASEL Execution Time: 37.11s\n",
      "WEASEL Memory Usage: 214.16 MB\n",
      "WEASEL Precision: 0.86\n",
      "WEASEL Accuracy: 0.85\n",
      "WEASEL F1 Score: 0.84\n",
      "WEASEL ROC-AUC Score (Macro): 0.99\n",
      "WEASEL ROC-AUC Score (Micro): 0.99\n",
      "MUSE Execution Time: 62.04s\n",
      "MUSE Memory Usage: 237.85 MB\n",
      "MUSE Precision: 0.88\n",
      "MUSE Accuracy: 0.86\n",
      "MUSE F1 Score: 0.86\n",
      "MUSE ROC-AUC Score (Macro): 0.99\n",
      "MUSE ROC-AUC Score (Micro): 0.99\n",
      "ShapeDTW Execution Time: 268.74s\n",
      "ShapeDTW Memory Usage: 184.42 MB\n",
      "ShapeDTW Precision: 0.81\n",
      "ShapeDTW Accuracy: 0.80\n",
      "ShapeDTW F1 Score: 0.79\n",
      "ShapeDTW ROC-AUC Score (Macro): 0.90\n",
      "ShapeDTW ROC-AUC Score (Micro): 0.90\n",
      "KNeighborsTimeSeriesClassifier Execution Time: 3531.91s\n",
      "KNeighborsTimeSeriesClassifier Memory Usage: 46.13 MB\n",
      "KNeighborsTimeSeriesClassifier Precision: 0.71\n",
      "KNeighborsTimeSeriesClassifier Accuracy: 0.68\n",
      "KNeighborsTimeSeriesClassifier F1 Score: 0.67\n",
      "KNeighborsTimeSeriesClassifier ROC-AUC Score (Macro): 0.84\n",
      "KNeighborsTimeSeriesClassifier ROC-AUC Score (Micro): 0.84\n",
      "Catch22Classifier Execution Time: 7.65s\n",
      "Catch22Classifier Memory Usage: 119.34 MB\n",
      "Catch22Classifier Precision: 0.83\n",
      "Catch22Classifier Accuracy: 0.82\n",
      "Catch22Classifier F1 Score: 0.81\n",
      "Catch22Classifier ROC-AUC Score (Macro): 0.99\n",
      "Catch22Classifier ROC-AUC Score (Micro): 0.99\n",
      "FreshPRINCEClassifier Execution Time: 1724.42s\n",
      "FreshPRINCEClassifier Memory Usage: 601.50 MB\n",
      "FreshPRINCEClassifier Precision: 0.87\n",
      "FreshPRINCEClassifier Accuracy: 0.86\n",
      "FreshPRINCEClassifier F1 Score: 0.86\n",
      "FreshPRINCEClassifier ROC-AUC Score (Macro): 0.99\n",
      "FreshPRINCEClassifier ROC-AUC Score (Micro): 0.99\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each classifier\n",
    "for classifier in classifiers:\n",
    "    classifier_name = type(classifier).__name__\n",
    "    # Use the resampled data if resampling was done, else use the original data\n",
    "    if resampling_done:\n",
    "        exec_time, max_mem_usage, precision, accuracy, f1_score_val, roc_auc_macro, roc_auc_micro, confusion = \\\n",
    "            evaluate_classifier(classifier, X_train_flat, X_test_flat, y_train, y_test)\n",
    "\n",
    "    else:\n",
    "        exec_time, max_mem_usage, precision, accuracy, f1_score_val, roc_auc_macro, roc_auc_micro, confusion = \\\n",
    "            evaluate_classifier(classifier, X_train_flat_resampled, X_test_flat, y_train_resampled, y_test)\n",
    "\n",
    "\n",
    "    results[\"Classifier\"].append(classifier_name)\n",
    "    results[\"Execution Time\"].append(exec_time)\n",
    "    results[\"Memory Usage\"].append(max_mem_usage)\n",
    "    results[\"Precision\"].append(precision)\n",
    "    results[\"Accuracy\"].append(accuracy)\n",
    "    results[\"F1 Score\"].append(f1_score_val)\n",
    "    results[\"ROC-AUC Score (Macro)\"].append(roc_auc_macro)\n",
    "    results[\"ROC-AUC Score (Micro)\"].append(roc_auc_micro)\n",
    "    results[\"Confusion Matrix\"].append(confusion)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{classifier_name} Execution Time: {exec_time:.2f}s\")\n",
    "    print(f\"{classifier_name} Memory Usage: {max_mem_usage:.2f} MB\")\n",
    "    print(f\"{classifier_name} Precision: {precision:.2f}\")\n",
    "    print(f\"{classifier_name} Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"{classifier_name} F1 Score: {f1_score_val:.2f}\")\n",
    "    print(f\"{classifier_name} ROC-AUC Score (Macro): {roc_auc_macro:.2f}\")\n",
    "    print(f\"{classifier_name} ROC-AUC Score (Micro): {roc_auc_micro:.2f}\")\n",
    "\n",
    "\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_prob = classifier.predict_proba(X_test_flat)\n",
    "        y_test_bin = label_binarize(y_test, classes=np.unique(y_train))\n",
    "        n_classes = y_test_bin.shape[1]\n",
    "\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_prob[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        fpr_dict[classifier_name] = fpr\n",
    "        tpr_dict[classifier_name] = tpr\n",
    "        roc_auc_dict[classifier_name] = roc_auc"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-15T23:28:29.947588500Z"
    }
   },
   "id": "c3be9b301c0d217c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot ROC-AUC Curves\n",
    "# Define the number of columns and rows you want\n",
    "num_cols = 4  # Fewer columns\n",
    "num_rows = 6  # More rows to accommodate all classifiers, assuming 21 classifiers\n",
    "\n",
    "# Calculate figure size dynamically based on the number of columns and rows\n",
    "# Each subplot will be of size (4, 4) for example, but you can adjust this as needed\n",
    "subplot_size_width = 4\n",
    "subplot_size_height = 4\n",
    "fig_width = subplot_size_width * num_cols\n",
    "fig_height = subplot_size_height * num_rows\n",
    "\n",
    "# Initialize the figure with the calculated dimensions\n",
    "plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "# Create the ROC AUC plots\n",
    "for i, classifier_name in enumerate(results[\"Classifier\"]):\n",
    "    ax = plt.subplot(num_rows, num_cols, i + 1)\n",
    "    for j in range(n_classes):\n",
    "        ax.plot(fpr_dict[classifier_name][j], tpr_dict[classifier_name][j], lw=2,\n",
    "                label=f'Class {j} (AUC = {roc_auc_dict[classifier_name][j]:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC AUC for {classifier_name}')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "# Adjust the spacing between subplots and the top edge of the figure\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3, top=0.9)\n",
    "\n",
    "# Add an overall title\n",
    "plt.suptitle(f'{dataset_name} ROC AUC Curves', fontsize=20, y=0.98)\n",
    "\n",
    "# Save the figure with enough room for the suptitle\n",
    "plt.tight_layout()  # This adjusts subplot params so that the subplots fit into the figure area.\n",
    "plt.subplots_adjust(top=0.95)  # Adjust this value to increase the space for the title.\n",
    "plt.suptitle(f\"{dataset_name} ROC AUC Curves\", fontsize=16)\n",
    "plt.savefig(f\"{dataset_name}_ROC_AUC_curves.png\", bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "72a9d0b82e33ef96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_roc_auc_curves_macro(fpr_dict, tpr_dict, roc_auc_dict, classifiers, n_classes, dataset_name=dataset_name):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    colors = cycle(['midnightblue', 'indianred', 'green', 'purple', 'orange', 'brown', 'pink', 'gray', 'olive', 'cyan', 'mediumaquamarine', 'chocolate', 'palegreen', 'antiquewhite', 'tan', 'darkseagreen', 'aquamarine', 'cadetblue', 'powderblue', 'thistle', 'palevioletred'])\n",
    "\n",
    "    for (classifier_name, color) in zip(classifiers, colors):\n",
    "        fpr = fpr_dict[classifier_name]\n",
    "        tpr = tpr_dict[classifier_name]\n",
    "        roc_auc = roc_auc_dict[classifier_name]\n",
    "\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])  # Use np.interp instead of interp\n",
    "        mean_tpr /= n_classes\n",
    "\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label=f'macro-average ROC curve of {classifier_name} (area = {roc_auc[\"macro\"]:.2f})',\n",
    "                 color=color, linestyle='-', linewidth=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{dataset_name} Macro-average ROC curve per classifier')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure with the dataset name in the filename\n",
    "    filename = f\"{dataset_name}_macro_average_roc_curve.png\"\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    plt.close()  # Close the figure to free memory\n",
    "\n",
    "# Call the function with the appropriate parameters\n",
    "plot_roc_auc_curves_macro(fpr_dict, tpr_dict, roc_auc_dict, results[\"Classifier\"], n_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c0723088df26db0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to plot results\n",
    "def plot_results(results, metric, title, color):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(results[\"Classifier\"], results[metric], color=color)\n",
    "    plt.xlabel('Classifiers')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_results_improved(results, metric, dataset_name, color, ylabel=None):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.bar(results[\"Classifier\"], results[metric], color=color)\n",
    "    plt.xlabel('Classifiers')\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel)\n",
    "    title = f\"{dataset_name} {metric} Comparison\"\n",
    "    plt.title(title)\n",
    "    if metric == \"Execution Time\":\n",
    "        max_execution_time = max(results[metric])\n",
    "        plt.ylim(0, max_execution_time * 1.1)\n",
    "    else:\n",
    "        plt.ylim(0, max(results[metric]) * 1.1)  # Adjust for other metrics as well\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{dataset_name}_{metric}.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Apply the improved plotting function for each metric you want to plot\n",
    "plot_results_improved(results, \"Accuracy\", dataset_name, \"chocolate\")\n",
    "plot_results_improved(results, \"ROC-AUC Score (Macro)\", dataset_name, \"saddlebrown\")\n",
    "plot_results_improved(results, \"Execution Time\", dataset_name, \"sandybrown\", ylabel=\"Time (s)\")\n",
    "plot_results_improved(results, \"Memory Usage\", dataset_name, \"peachpuff\", ylabel=\"Space (MB)\")\n",
    "plot_results_improved(results, \"Precision\", dataset_name, \"peru\")\n",
    "plot_results_improved(results, \"F1 Score\", dataset_name, \"sienna\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "543df7a92ff9fca9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot confusion matrices together\n",
    "num_classifiers = len(results[\"Classifier\"])\n",
    "num_cols = 7\n",
    "num_rows = -(-num_classifiers // num_cols)  # Ceiling division\n",
    "\n",
    "plt.figure(figsize=(20, 4 * num_rows))\n",
    "for i, classifier_name in enumerate(results[\"Classifier\"]):\n",
    "    plt.subplot(num_rows, num_cols, i + 1)\n",
    "    plt.imshow(results[\"Confusion Matrix\"][i], interpolation='nearest', cmap=plt.cm.Oranges)\n",
    "    plt.title(f'{classifier_name}')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    tick_marks = np.arange(len(np.unique(y_train)))\n",
    "    plt.xticks(tick_marks, tick_marks, rotation=45)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "# Adjust the spacing of the subplots to make room for the suptitle\n",
    "plt.subplots_adjust(top=0.85)  # You may need to adjust this value\n",
    "plt.suptitle(f\"{dataset_name} Confusion Matrices\", fontsize=16)\n",
    "\n",
    "# Save the figure with enough room for the suptitle\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # You may need to adjust these values\n",
    "plt.savefig(f\"{dataset_name}_Confusion_Matrices.png\", bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5489c456b227b024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
